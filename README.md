# Guidance for Building a Real Time Bidder for Advertising on AWS

### RTB Code Kit Deployment Guide
# Guidance for Building a Real Time Bidder for Advertising on AWS

### RTB Code Kit Deployment Guide

## Introduction

Real-time advertising platforms operate round-the-clock, processing millions of transactions, running bidding, ad serving, and verification workloads at ultra-low latency and high throughput. 

In this industry, expenses associated with infrastructure costs such as compute, databases, and networking significantly impact profit margins. Consequently, ad tech firms are driven to constantly maximize the price-performance of their platforms.

This AWS Solution, "Guidance for Building a Real Time Bidder for Advertising on AWS" presents a deployable reference architecture that leverages open-source technologies to showcase the "art of the possible" in the advertising space. This blueprint empowers demand-side platforms (DSP) to develop advanced, innovative, and smart bidding services. 

The Real-Time Bidder Solution on AWS consists of 5 modules: 

1. **Data generator**: Generates synthetic data for the device, campaign, budget, and audience. The size of each table can be defined in the configuration file, it is recommended that one billion devices be generated for the devices table, one million campaigns and associated budgets, and one hundred thousand audiences.

2. **Load generator**: Generates artificial bid requests (based on the OpenRTB 2.5 or OpenRTB 3.0 standard).

3. **Bidder**: Receives and parses bid requests, searches for the device and associated metadata in the device table, selects the best fitting campaign, determines the bid price for the ad opportunity, constructs the bid request/response, and writes the bid request/response to a data pipeline. 

4. **Data repository**: Database for storing device, campaign, budget, and audience data. Supports either DynamoDB or Aerospike.

5. **Data Pipeline**: Receives, aggregates and writes the bid request and response to a data repository. Includes tools to ingest the metrics generated by the bidder and displays the results for evaluation.

6. **Monitoring Tools**: Uses Grafana and Prometheus on Amazon Elastic Kubernetes Service (EKS) to collect and display application logs such as the bid requests per second and latency.

## Prerequisites

1. Python 3

2. Ensure the AWS CLI and AWS CDK are installed on your local machine. These are used to access the EKS cluster and Grafana Dashboards:
    * [AWS CLI V2](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
    * [AWS CDK](https://docs.aws.amazon.com/cdk/v2/guide/getting_started.html#:~:text=The%20actual%20package%20name%20of%20the%20main%20CDK%20package%20varies%20by%20language.) 

3. The following packages are required and can be installed using the helper script [client-setup.sh](./client-setup.sh):
    * [Helm 3.8.2](https://helm.sh/)
    * [kubectl 1.21.0](https://kubernetes.io/docs/tasks/tools/#kubectl)
    * [JQ](https://stedolan.github.io/jq/download/)

4. Create an IAM user with the following permissions in the AWS account that you will be deploying this solution:
    * Administrator and Programmatic access
    * GitHub repository creation and write access

5. The [CDK code](./cdk/pipeline) deploys a code pipeline and automates the deployment of the components. The code pipeline requires a supported Git repository (GitHub or Bitbucket) as the source. In this example we are using GitHub. Follow these steps to create a GitHub repo:
    1. Create a new GitHub repo as a fork of the opensource. Follow the instructions [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/fork-a-repo)
    2. Create an OAuth Personal Access Token with read access to the repository and read/write access to Webhooks. Follow instructions [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens#creating-a-fine-grained-personal-access-token).

6. Create a new secret in AWS Secrets Manager via the AWS Console to store the GitHub Personal Access Token (PAT) from the step above. Ensure you are in the correct region:
    1. Login to the AWS console and navigate to AWS Secrets Manager.
    2. Select `Store a new secret`, click `Other type of secret`, use the `plaintext` option and save the GitHub PAT from the step above. 
        >NOTE Don’t use the key value pair nor the Json format. 
    3. Enter a name for the secret. This name is required in the cdk context file later on.
    4. Leave the rest as default and store the secret.

### Service Limits (not required atm)

Increase the following service limits via Service Quotas section in AWS Console.
* DynamoDB table Read Capacity Unit (RCU) limit should be increased to 150,000 RCU's from default 40,000 RCU's. The limit is called `Table-level read throughput limit` under the `Amazon DynamoDB` service quota.
* Kinesis shards limit should be increased to 2048. The limit is called `Shard per Region` under `Amazon Kinesis Data Streams` service quota.

## Architecture Diagram

![Architecture](./images/RTBCodekitArchitecture.png)

## Deployment of DSP Infrastructure and Application

If you are looking for instructions to provision publisher (SSP) infrastructure, go to the section "Deployment of Publisher (SSP) Infrastructure and Load Generator".
The following steps describe deployment of the infrastructure that will deploy responder application capable of accepting bid requests and responding with contrived data (generated with the datagen tool). You can run local load testing of this cluster (in-cluster load test) if needed. Along with the application, the cluster will have the observability stack based on prometheus/grafana. 

1. Clone this repo to your local machine.

3. Configure your settings by creating a `.env` file in the root (use `envtemplate` as template). Update the `STACK_NAME`,`STACK_VARIANT` (DynamoDB/Aerospike/DynamoDBBasic) and the rest of the variables in the `.env` file. If you don't have Heimdall target yet, set TARGET to TARGET_LOCAL. Leave the BUILD_SPEC as `buildspec.yml`. 
**Important:** make sure STACK_NAME is unique as it creates a bucket with that name. Also remove any trailing comments, like `# set this to a  unique name such as rtbkit-<your-alias>-<region>`. 
    ```
    cp envtemplate .env
    cat .env

    STACK_NAME=rtbkit-<your-user-alias>-<region>
    AWS_REGION=us-east-1
    STACK_VARIANT=DynamoDbBasic
    REPO_OWNER=shapirov103
    REPO_NAME=guidance-for-building-a-real-time-bidder-for-advertising-on-aws
    REPO_BRANCH=main
    BUILD_SPEC=buildspec.yml
    TARGET_LOCAL="http://bidder/bidrequest"
    TARGET_HEIMDALL="https://<your rtb app>.<acct>.<region>.dataplane.rtb.mpofxdevmu.aws.dev/link/<link-id>/bidrequest"
    TARGET_PUBLIC_NLB="http://<your-nlb-dns>.amazonaws.com/bidrequest"
    TARGET=$(TARGET_HEIMDALL)
    ```
4.  Check if python3 and the python3 virtual environment are installed on your machine if not install python3:
    ```
    python3 --version
    ```
5.  Set up CDK (installs the requirements and boto3 libraries and bootstraps):
    ```
    make cdk@setup
    # validate the settings
    make cdk@list
    ```
6. Deploy the CDK stack:
    ```
    make cdk@deploy
    ```
7. CDK will deploy the resources as shown below:

    ![CDK Deployment](./images/CDKDeployment-2.png)
    ![CDK Deployment](./images/CDKDeployment.png)

8. On successful deployment you will see as following
    ```
    ✅ RTBBuildStack
    ```

9. After the CDK deployment is complete, a CodeBuild project will be provisioned ready to build and deploy both infrastructure and the `Real-Time-Bidding Solution` in your AWS Account using CloudFormation. 

10. Kick off the CodeBuild build by running the following command:

```sh
cd ../.. # return to the project root
aws codebuild start-build --project-name "rtb-build-project"
```
This will take approximately twenty minutes. You can navigate to AWS Console (UI) and find the CodeBuild project named `rtb-build-prpject` and observe the execution of the latest run through the logs. 

Once successful you will see:

    ![Build Success](./images/buildsuccess.png)

11. (Optional for Aerospike variant) Open AWS console and navigate to EKS service. Locate the cluster deployed by the stack. Navigate to Add-ons tab and install the Amazon EBS CSI Driver add on:
    > **IMPORTANT**: Select `Optional configuration schema` and click select `Override` for the `Conflict resolution method` 
    
    ![EKS Add on](./images/eks-addon.png)

12. Once the deployment is completed go to the CloudFormation console and navigate to root stack (this will be the stack with name that you have provided in cdk.json file in step 4). Go to Outputs tab and copy `ApplicationStackName`, `ApplicationStackARN`, `EKSAccessRoleARN`, `EKSWorkerRoleARN`. We will be using them in the following steps.

13. Ensure the pre-requisites (`Helm`, `Kubectl` and `jq`) are installed on the local/client machine as per the pre-requisites section above.
    > Tip: The pre-requisites can be installed using the shell script [client-setup.sh](./client-setup.sh). Navigate to the root directory and change the script permissions `chmod 700 client-setup.sh` before running the script.

    >NOTE: Commands for steps 15 - 22 are included in a shell script [run-benchmark.sh](./run-benchmark.sh). Navigate to the directory and change the script permissions `chmod 700 client-setup.sh` if required before running the script.

14. Now run the `make` command to access the EKS cluster by:
    >NOTE: This command has to be run from the root folder of the code repository:

    ```
    make eks@use
    ```

15. Run the following command to list the pods in cluster. You should see the pods as shown in the screenshot below.
    ```
    kubectl get pods
    ```

    ![Get Pods](./images/getpods.png)

17. If you plan to run benchmarks using distributed setup proceed to section "How to use the RTB guidance with Heimdall". If you would like to run a load test internally (inside the Kubernetes cluster) you can start local benchmarks by initiating the load-generator along with the parameters.
    ```
    make benchmark@cleanup
    make benchmark@run TIMEOUT=100ms NUMBER_OF_JOBS=1 RATE_PER_JOB=200 NUMBER_OF_DEVICES=10000 DURATION=500s
    ```
    _You can supply following parameters to load-generator and perform benchmarks_
    ```
    TIMEOUT=100ms        # Request timeout (default 100ms)
    DURATION=500s        # duration of the load generation
    RATE_PER_JOB=5000    # target request rate for the load generator
    NUMBER_OF_DEVICES=10 # number of devices IFAs to use in bid request
    NUMBER_OF_JOBS=1     # number of parallel instances of the load generator
    SLOPE=0              # slope of requests per second increase (zero for a constant rate; see <https://en.wikipedia.org/wiki/Slope>)
    ENABLE_PROFILER=1    # used to start profiling session, leave unset to disable
    ```

18. To observe metrics and dashboards in Grafanca:
    ```
    kubectl port-forward svc/prom-grafana 8080:80
    ```

19. On your local/client instance open the URL [localhost:8080](http://localhost:8080) access Grafana Dashboard. 
Use the following credentials to login (Turn off enhanced tracking if you are using Firefox)

    ```
    username: admin
    Password: prom-operator
    ```

    ![Grafana login](./images/aws-rtb-grafana-login.png)

20. Once you login, click on the dashboard button on the left hamburger menu and select manage as shown in the figure below.

    ![Dashboards](./images/aws-rtb-grafana-dashboards.png)

21. Search and access 'bidder' dashboard from the list.

    ![Bidder](./images/aws-rtb-grafana-bidder.png)

22. You will see the bid request that are being generated on the right side and latency on the left side of the dashboard as shown in the figure below.

    ![Benchmark](./images/benchmarksresults.png)

The metrics include:

* Bid requests generated
* Bid requests received
* No Bid responses
* Latency on 99, 95 and 90 percentiles

These benchmarks help demonstrate the Real-time-bidder application performance on AWS Graviton instances.

>IMPORTANT :  After running the benchmark delete the stack to avoid incurring AWS cloud costs

>TIP: You can also change the DynamoDB and kinesis configurations to use on-demand compute instead of provisioned. This will reduce the cost of running the resources in the stack. In addition to this go to EKS and set the max and desired count of node in all node groups to 0. This will automatically terminate all the running EC2 instances launched by the stack. These two changes could reduce the running cost to 1/10th. As of June 2024, the average running cost of the stack with basic configuration is around $800/day in us-west-2

# Notes
* You can disable the data pipeline by setting KINESIS_DISABLE: "true" in deployment/infrastructure/charts/bidder/values.yaml file
* We have used unsafe pointers to optimize the heap allocation and are not converting the pointer types in the code. If this code is used in production, we strongly recommend you to look at your current code and set the pointer type in ./apps/bidder/code/stream/producer/batch_compiler.go file.
* For the ease of deployment, we have pre-configured user credentials for Grafana Dashboard. This is not a best practice and we strongly recommend you to configure access via AWS IAM/SSO. (./deployment/Infrastructure/deployment/prometheus/prometheus-values.yaml, ./tools/benchmark-utils/function.sh)
* Since CloudTrail is enabled on the AWS account by default. we strongly recommend not to disable it. We have not made any CloudTrail configuration on the code kit to enable it if it is disabled.
* Use [update-node-group-size.sh](./update-node-group-size) to downsize the EKS cluster to zero to save compute costs while testing
* Use [check-pod-logs.sh](./check-pod-logs.sh) to get the bidder logs for troubleshooting purposes

# How to use NLB with RTB Kit

The following are instructions to create either internal or external NLB for load testing. These can be used to simulate external traffic to the cluster and to compare latency between internal, external and direct connection to the cluster (e.g. through Heimdall).

## Internal NLB
1. Deploy internal NLB: `kubectl apply -f deployment/infrastructure/deployment/bidder-nlb.yaml`
2. Get DNS hostname of the loadbalancer: `kubectl get services bidder-nlb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'`

## External NLB
1. Deploy external NLB: `kubectl apply -f deployment/infrastructure/deployment/bidder-external.yaml`
2. Get DNS hostname of the loadbalancer: `kubectl get services bidder-external -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'`

# How to use the RTB guidance with Heimdall

## Responder (bidder application)

The below steps do not reflect the EKS Endpoints approach yet, these steps will be included shortly. The current setup is showing the steps for internal NLB setup with Heimdall. 

Provision internal NLB for your responder app (see "How to use NLB with RTB Kit"). The NLB will be placed in the private subnets, marked with tag `kubernetes.io/role/internal-elb: 1` which is set by the infrastructure provisioning (through CDK).

Run the following command to get subnets and security groups required to onboard your responder application:

```
aws elbv2 describe-load-balancers --names rtb-bidder-nlb
```

The command above assumes that the name of the NLB is `rtb-bidder-nlb`. If you modified it, please specify the name you applied. The output will contain DNS name, subnet ids and security groups that you will use for Heimdall onboarding of the responder app:

```
            "LoadBalancerArn": "arn:aws:elasticloadbalancing:us-east-1:<>",
            "DNSName": "rtb-bidder-nlb-<postfix>.elb.us-east-1.amazonaws.com",
            "LoadBalancerName": "rtb-bidder-nlb",
            "Scheme": "internal",
            "VpcId": "vpc-id",
            "State": {
                "Code": "active"
            },
            "Type": "network",
            "AvailabilityZones": [
                {
                    "ZoneName": "us-east-1a",
                    "SubnetId": "<subnet-1-id>",
                    "LoadBalancerAddresses": []
                },
                {
                    "ZoneName": "us-east-1c",
                    "SubnetId": "<subnet-2-id>",
                    "LoadBalancerAddresses": []
                },
                {
                    "ZoneName": "us-east-1b",
                    "SubnetId": "<subnet-3-id>",
                    "LoadBalancerAddresses": []
                }
            ],
            "SecurityGroups": [
                "sg-003c8e171aa159548",
                "sg-0a92e9c67ae598adb"
            ],
```

## Deployment of Publisher (SSP) Infrastructure and Load Generator

Your publisher infrastructure requires the following:

1. AWS Account (generally, it is a separate account from the bidder application)
2. VPC 
3. EKS Cluster to deploy the publisher app (emulated by the load generator in this solution)

We have prepared commands for you to accomplish steps #2 and #3. 

1. Configure your settings by creating a `.env` file in the root (use `envtemplate` as template). Update the `STACK_NAME`,`STACK_VARIANT` (DynamoDB/Aerospike/DynamoDBBasic) and the rest of the variables in the `.env` file. Set TARGET to the url of the DSP application to send to the traffic. 
**Important:** Modify the BUILD_SPEC to `buildspec-loadgen.yml`. Also make sure STACK_NAME is unique. Remove any trailing comments, like `# set this to a  unique name such as rtbkit-<your-alias>-<region>`. 
    ```
    cp envtemplate .env
    cat .env

    STACK_NAME=rtbkit-<your-user-alias>-<region>
    AWS_REGION=us-east-1
    STACK_VARIANT=DynamoDbBasic
    REPO_OWNER=shapirov103
    REPO_NAME=guidance-for-building-a-real-time-bidder-for-advertising-on-aws
    REPO_BRANCH=main
    BUILD_SPEC=buildspec-loadgen.yml
    TARGET_LOCAL="http://bidder/bidrequest"
    TARGET_HEIMDALL="https://<your rtb app>.<acct>.<region>.dataplane.rtb.mpofxdevmu.aws.dev/link/<link-id>/bidrequest"
    TARGET_PUBLIC_NLB="http://<your-nlb-dns>.amazonaws.com/bidrequest"
    TARGET=$(TARGET_HEIMDALL)

    ```

2. Start a new terminal session and navigate to the target account by setting AWS credentials and context to use it (e.g. switching AWS CLI profile). Starting a new session is required in order avoid context pollution from your previous runs. 

3. Check if python3 and the python3 virtual environment are installed on your machine if not install python3:
    ```
    python3 --version
    ```

4. Set up CDK (installs the requirements and boto3 libraries and bootstraps):
    ```
    make cdk@setup
    # validate the settings
    make cdk@list
    ```
5. Deploy the CDK stack:
    ```
    make cdk@deploy
    ```
6. CDK will deploy the resources as shown below:

    ![CDK Deployment](./images/CDKDeployment-2.png)
    ![CDK Deployment](./images/CDKDeployment.png)

7. On successful deployment you will see as following
    ```
    ✅ RTBBuildStack
    ```

8. After the CDK deployment is complete, a CodeBuild project will be provisioned ready to build and deploy both infrastructure and the `Real-Time-Bidding Solution` in your AWS Account using CloudFormation. 

9. Kick off the CodeBuild build by running the following command:

```sh
cd ../.. # return to the project root
aws codebuild start-build --project-name "rtb-build-project"
```
This will take approximately twenty minutes. You can navigate to AWS Console (UI) and find the CodeBuild project named `rtb-build-prpject` and observe the execution of the latest run through the logs. 

Once successful you will see:

    ![Build Success](./images/buildsuccess.png)

10. Run `make publisher-eks@provision` in the publisher account. This will provision a new VPC with a well-architected EKS cluster in Auto Mode. The cluster has an ARM Node Pool, managed Karpenter and a number of add-ons out of the box.

11. The above command will automatically update your .kube/config. You can validate access by running:
```
kubectl config current-context # should produce something like <username>@publisher-eks.us-east-1.eksctl.io
kubectl get nodepools
```

You will see nodepools `system` and `generic-workloads`. The latter is a pool based on Graviton instances to run load testing. 

12. Onboard the private subnets of the created VPC as part of your publisher application in Heimdall. 

```
aws eks describe-cluster --name publisher-eks --query "cluster.resourcesVpcConfig.subnetIds" --output text
```

Run `create-requester-rtb-app` (using onboarding guide).

13. In your `.env` file set variable `TARGET_HEIMDALL` to the link that you established with the responder application. Then set the `TARGET` to point to the `TARGET_HEIMDALL`.

```
TARGET_HEIMDALL="https://<rtb-app-id>.<acct>.<region>.dataplane.rtb.mpofxdevmu.aws.dev/link/<link-id>/bidrequest"
TARGET=$(TARGET_HEIMDALL)
```

14. Make sure that the current `kubectl config current-context` points to the publisher-eks cluster. (See step 3). Kick off the load test by running one of the benchmark targets.

```
# run a small batch load test
make benchmark@codekit
```

Note: in this setup benchmarks are running in distributed mode on a remote cluster accessing the responder app using Heimdall provided network and broker. 

15. If you performed all of these steps on the same machine then you have more than a single context in your kubernetes config. Here are some commands that will help you navigate in between the contexts. 

See all available contexts. You should observe the cluster with the `STACK_NAME` that you provided, which corresponds to your responder app, as well as the `publisher-eks` cluster. 
```
kubectl config get-contexts
```

To switch context to the target cluster, copy its name (e.g. `arn:aws:eks:us-east-1:<AWS_ACCOUNT>:cluster/<STACK_NAME>`) and use the following:

```
kubectl config use-context  arn:aws:eks:us-east-1:<AWS_ACCOUNT>:cluster/<STACK_NAME>
```

Note: for cross account setup you need to authenticate to the target account before you can access the cluster. this can be achieved by changing the profile, authenticating using SSO, pasting session scope credentials as env vairables, etc. 


